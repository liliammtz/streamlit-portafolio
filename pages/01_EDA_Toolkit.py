import streamlit as st

st.set_page_config(page_title="EDA", page_icon="üìä", layout="wide")
st.title("üìä EDA Toolkit")

with st.sidebar:
    st.markdown("### üëã About me")
    st.write(
        "Data Scientist with a strong background in **forecasting**, **business intelligence**, and **ML-powered analytics**. "
        "I specialize in building **end-to-end data products** ‚Äî from data pipelines and predictive models in **Snowflake/SQL** "
        "to polished **Streamlit apps** used daily by business teams. "
        "Passionate about turning raw data into clear, actionable insights that support **strategic decision-making**."
    )

    st.divider()
    st.markdown("**Tools**")
    st.page_link("Main.py", label="Home", icon="üè†")
    # These will work once you add the multipage files under /pages
    st.page_link("pages/01_EDA_Toolkit.py", label="EDA Toolkit", icon="üìä")
    
    st.divider()
    st.markdown("**Contact**")
    st.markdown("- GitHub: [@liliam-mtz](https://github.com/)")
    st.markdown("- LinkedIn: [Liliam Mart√≠nez](https://www.linkedin.com/in/liliammtz/)")
    st.markdown("- Email: [liliammtzfdz@gmail.com](mailto:liliammtzfdz@gmail.com)")

# =========================
# Paste Archivo
# =========================

import io
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import plotly.express as px
import streamlit as st
from sklearn.decomposition import PCA
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# =========================
# 0) Datos de ejemplo o subida
# =========================
@st.cache_data
def sample_df(n=500, seed=42):
    rng = np.random.default_rng(seed)
    dates = pd.date_range("2024-01-01", periods=n, freq="D")
    df = pd.DataFrame({
        "id": np.arange(1, n+1),
        "date": dates,
        "num_a": rng.normal(100, 20, n).round(2),
        "num_b": rng.gamma(2.0, 15.0, n).round(2),
        "cat_x": rng.choice(["A","B","C"], size=n, p=[0.5,0.3,0.2]),
        "cat_y": rng.choice(["Norte","Sur"], size=n),
        "flag": rng.choice([0,1], size=n, p=[0.7,0.3])
    })
    # Introducimos algunos NaNs y duplicados
    df.loc[rng.choice(n, size=10, replace=False), "num_a"] = np.nan
    df = pd.concat([df, df.iloc[[5]]], ignore_index=True)  # un duplicado
    return df

uploaded = st.file_uploader("Sube CSV/Excel para analisis", type=["csv","xlsx"])
if uploaded is not None:
    if uploaded.name.endswith(".csv"):
        df = pd.read_csv(uploaded)
    elif uploaded.name.endswith(".xslx"):
        df = pd.read_excel(uploaded)
    else:
        st.warning("Solo estan permitidos archivos de CSV o XLSX")
else:
    df = sample_df()

st.write("**Vista previa**")
st.dataframe(df.head())



import streamlit as st
import pandas as pd
import numpy as np
import io
import plotly.express as px
import plotly.figure_factory as ff
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# =========================
# Tabs principales
# =========================
t0, t1, t3, t4, t5, t6 = st.tabs([
    "üìä Calidad & estructura",
    "üìà Univariado (Descriptivas)",
    #"üì¶ Distribuciones (Skewness & Kurtosis)",
    "üîó Relaciones multivariadas",
    "üö® Outliers & Anomal√≠as",
    "üß© Categ√≥ricas & Balance de clases",
    "‚è≥ Series de tiempo"
])

# =========================
# üìä Calidad & estructura
# =========================
with t0:
    st.subheader("Informaci√≥n b√°sica")
    c0, c1 = st.columns(2)
    c0.write(
        """
        ### üîé Paso 1: Evaluar la calidad de los datos  

        Antes de profundizar en el an√°lisis, es fundamental revisar la **calidad del dataset**.  
        Algunas preguntas clave que debemos responder son:  

        - üìè **¬øDe qu√© tama√±o es el dataset?** (filas y columnas)  
        - üßæ **¬øQu√© tipos de datos contiene?** (num√©ricos, categ√≥ricos, fechas, texto, etc.)  
        - üè∑Ô∏è **¬øQu√© significan esas variables?** (interpretaci√≥n de las columnas)  
        - ‚ôªÔ∏è **¬øExisten valores duplicados o inconsistentes?**  
        - ‚ö†Ô∏è **Hay valores faltantes (nulos) que debamos tratar?**  
        """
    )

    c1.info(
            '‚ÄúNo data is clean, but most is useful.‚Äù  \n'
            '‚Äî [Dean Abbott, Co-founder and Chief Data Scientist at SmarterHQ]'
        )
        
    with c1:
        sc1, sc2 = st.columns(2)
        sc1.metric("Filas", df.shape[0])
        sc1.code("""filas = df.shape[0]""", language="python")
        sc2.metric("Columnas", df.shape[1])
        sc2.code("""columnas = df.shape[1]""", language="python")
        
    
    st.divider()
    c0, c1 = st.columns(2)
    c1.subheader("Tipos de datos")
    c1.write(
        """
        En Python existen los siguientes tipos de datos:

        - **Para texto:** `str`  
        - **Para datos num√©ricos:** `int`, `float`, `complex`  
        - **Para secuencias:** `list`, `tuple`, `range`  
        - **Para mapping:** `dict`  
        - **Para sets:** `set`, `frozenset`  
        - **Para booleanos (True/False):** `bool`  
        - **Para binarios:** `bytes`, `bytearray`, `memoryview`  
        - **Para nulos:** `NoneType`
        """
    )

    c0.code("""pd.DataFrame(df.dtypes, columns=["dtype"])""", language="python")
    c0.write(pd.DataFrame(df.dtypes, columns=["dtype"]))
    st.divider()
    c0, c1 = st.columns(2)
    c0.subheader("Manejo de nulos")
    c0.write(
        """
        üîç En un dataset es com√∫n encontrar valores nulos debido a la naturaleza de los datos 
        o a limitaciones en la recolecci√≥n de la informaci√≥n. Sin embargo, cuando la cantidad 
        de valores faltantes es considerable, conviene aplicar t√©cnicas de **imputaci√≥n** para 
        no perder informaci√≥n valiosa.  

        Algunas estrategias habituales incluyen:  
        - üìä **Media o mediana** (para variables num√©ricas).  
        - üìà **Valor m√°s frecuente (moda)** (para variables categ√≥ricas).  
        - üîÑ **Forward/Backward fill** (en series temporales).  
        - ü§ñ **Modelos predictivos** (kNN, regresiones, etc.).  

        La elecci√≥n de la t√©cnica depende tanto del tipo de variable como del contexto 
        del an√°lisis.
        """
    )
    
    c1.code("""df.isna().sum().sum()""", language='python')
    nulos_total = df.isna().sum().sum()
    if nulos_total > 0:
        with c1:
            st.warning(f"‚ö†Ô∏è Se encontraron {nulos_total} valores nulos.")
            na = df.isna().sum().sort_values(ascending=False).to_frame("n_nulls")
            na["pct"] = (na["n_nulls"] / len(df)) * 100
            #!st.dataframe(na)
            fig_na = px.bar(
                na.reset_index(),
                x="index", y="n_nulls",
                title="Nulos por columna",
                labels={"index": "Columna", "n_nulls": "N√∫mero de nulos"}
            )
            st.plotly_chart(fig_na, use_container_width=True)
    else:
        with c1:
            st.success("‚úÖ No se encontraron valores nulos.")

    st.divider()
    c0, c1 = st.columns(2)
    c1.subheader("Registros duplicados")
    c1.write(
        """
        üîÅ En muchos datasets pueden aparecer **filas duplicadas**, ya sea por errores en la 
        captura de informaci√≥n, procesos de integraci√≥n de datos o registros repetidos en 
        distintas fuentes.  

        La presencia de duplicados puede sesgar los resultados del an√°lisis, ya que implica 
        contar la misma informaci√≥n m√°s de una vez.  

        Algunas estrategias comunes para tratarlos son:  
        - ‚ùå **Eliminar duplicados exactos** (`drop_duplicates`).  
        - üîç **Revisar duplicados parciales**, manteniendo solo la observaci√≥n m√°s reciente o 
        m√°s completa.  
        - üìä **Agrupar o consolidar registros** cuando representan la misma entidad.  

        El enfoque adecuado depender√° del contexto y de la importancia que tenga cada 
        registro dentro del an√°lisis.
        """
    )

    duplicados_total = df.duplicated().sum()
    c0.code("df.duplicated().sum()",language='python')
    if duplicados_total > 0:
        with c0:
            st.warning(f"‚ö†Ô∏è Se encontraron {duplicados_total} registros duplicados.")
            st.dataframe(df[df.duplicated(keep=False)].sort_values(list(df.columns)))
    else:
        with c0:
            st.success("‚úÖ No se encontraron duplicados.")

    st.info(
        """
        ### üìö Recursos √∫tiles  

        - [Python Data Types ‚Äì W3Schools](https://www.w3schools.com/python/python_datatypes.asp)  
        - [Data Imputation: A Comprehensive Guide (Medium)](https://medium.com/@ajayverma23/data-imputation-a-comprehensive-guide-to-handling-missing-values-b5c7d11c3488)  
        - [Imputation Methods ‚Äì scikit-learn](https://scikit-learn.org/stable/modules/impute.html)  
        """
    )

# =========================
# üìà Univariado (Descriptivas)
# =========================
with t1:
    st.markdown(
        """
        ### üìà An√°lisis univariado  
        El an√°lisis univariado permite explorar cada variable de manera individual, 
        entendiendo su **distribuci√≥n**, **variabilidad** y posibles valores at√≠picos.  
        Aqu√≠ se muestran estad√≠sticas descriptivas y visualizaciones por tipo de dato.
        """
    )

    # --- Estad√≠sticas descriptivas ---
    num_cols = df.select_dtypes(include=np.number).columns.tolist()
    cat_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()

    st.subheader("üìä Estad√≠sticas descriptivas")
    c1, c2 = st.columns(2)
    if num_cols:
        with c1:
            st.markdown("**Num√©ricas**")
            st.code("""df[num_cols].describe().T""",language='python')
            st.dataframe(df[num_cols].describe().T)
    if cat_cols:
        with c2:
            st.markdown("**Categ√≥ricas**")
            st.code("""df[cat_cols].describe().T""",language='python')
            st.dataframe(df[cat_cols].describe().T)
    # --- Num√©ricas (una sola gr√°fica combinada) ---
    st.divider()
    st.subheader("üìà Distribuci√≥n de variables num√©ricas")
    c1, c2 = st.columns([1.2, 1])
    
    with c1:
        st.write(
            "Explora la distribuci√≥n con **histograma** + **KDE** en una sola vista. "
            "**Tip:** Activa escala log si hay colas largas."
        )

        if num_cols:
            col_num = st.selectbox("Variable num√©rica", num_cols, key="num_univar")
            bins = st.slider("Bins", 10, 100, 40, key="bins_univar")
            use_log = st.toggle("Escala log en eje Y", value=False, key="log_univar")

            # Datos limpios
            serie = df[col_num].dropna().astype(float)

            # Gr√°fica combinada: histograma + KDE (Figure Factory)
            # Nota: ff.create_distplot ya combina histograma + curva KDE
            fig_dist = ff.create_distplot(
                [serie.values], [col_num],
                bin_size=(serie.max() - serie.min()) / bins if bins else None,
                show_hist=True, show_rug=False
            )

            # L√≠neas de referencia: media y mediana
            mean_v = float(serie.mean())
            median_v = float(serie.median())
            fig_dist.add_vline(x=mean_v, line_dash="dash", annotation_text=f"mean={mean_v:.2f}")
            #!fig_dist.add_vline(x=median_v, line_dash="dot", annotation_text=f"median={median_v:.2f}")

            # Escala log opcional en Y
            fig_dist.update_yaxes(type="log" if use_log else "linear")

            fig_dist.update_layout(title=f"Histograma + KDE: {col_num}", showlegend=False)
            st.plotly_chart(fig_dist, use_container_width=True)
        else:
            st.info("No hay columnas num√©ricas disponibles.")

    with c2:
        if num_cols:
            # Estad√≠sticos de apoyo (skew/kurtosis) para la variable seleccionada
            sel = df[col_num].dropna().astype(float)
            skew_v = float(sel.skew())
            kurt_v = float(sel.kurt())

            # Boxplot compacto para contexto (misma variable)
            st.markdown("**Boxplot (resumen de dispersi√≥n)**")
            box_fig = px.box(sel.to_frame(name=col_num), y=col_num)
            box_fig.update_layout(height=300, margin=dict(l=10, r=10, t=30, b=10))
            st.plotly_chart(box_fig, use_container_width=True)

            sc1, sc2 = st.columns(2)
            sc1.metric("Skewness", f"{skew_v:.2f}")
            sc2.metric("Kurtosis", f"{kurt_v:.2f}")

            st.markdown(
                """
                **Tips de interpretaci√≥n**  
                - **Skewness > 0**: cola a la derecha (pocos valores muy altos).  
                - **Skewness < 0**: cola a la izquierda.  
                - **Kurtosis** alta: colas pesadas (posibles outliers).  
                - Considera **mediana/IQR** si hay sesgo; puedes probar **log** en Y.
                """
            )

    st.divider()
    
    st.subheader("üìä Distribuci√≥n de variables categ√≥ricas")
         
    c1, c2 = st.columns(2) 
    with c2:    
        # --- Categ√≥ricas ---
        if cat_cols:
            
            
                col_cat = st.selectbox("Selecciona variable categ√≥rica", cat_cols)
                top_n = st.slider("Mostrar Top N categor√≠as", 5, 20, 10)
                
                counts = df[col_cat].value_counts(dropna=False).reset_index()
                counts.columns = [col_cat, "count"]
                counts["pct"] = counts["count"] / counts["count"].sum() * 100
                counts = counts.head(top_n)

                st.dataframe(counts)

                fig_bar = px.bar(
                    counts, x=col_cat, y="count",
                    text="pct", title=f"Top {top_n} categor√≠as: {col_cat}",
                    labels={"count": "Frecuencia"},
                    color_discrete_sequence=["#EF553B"]
                )
                fig_bar.update_traces(texttemplate="%{text:.1f}%", textposition="outside")
                st.plotly_chart(fig_bar, use_container_width=True)



    st.subheader("üî¢ Conteo de valores √∫nicos")
    st.dataframe(df.nunique(dropna=True).to_frame("Valores √∫nicos"))




# =========================
# üîó Relaciones multivariadas
# =========================
with t3:
    st.subheader("üîó Relaciones multivariadas")
    st.write("""### üîó Paso 3: Relaciones multivariadas  

En este paso buscamos entender **c√≥mo se relacionan varias variables entre s√≠**.  
Algunas preguntas clave que nos ayuda a responder son:  

- üìà **¬øQu√© variables se mueven juntas?**  
  (Correlaciones Pearson, Spearman, Kendall para detectar asociaciones lineales o mon√≥tonas).  

- üîç **¬øExisten relaciones no lineales o subgrupos escondidos?**  
  (Gr√°ficos de dispersi√≥n y densidad permiten ver heterocedasticidad, clusters o outliers).  

- üßÆ **¬øC√≥mo var√≠a una num√©rica seg√∫n una categ√≥rica?**  
  (Boxplots o violinplots muestran distribuci√≥n, medianas y valores extremos por categor√≠a).  

- ‚ö†Ô∏è **¬øHay multicolinealidad entre variables num√©ricas?**  
  (Correlaciones altas sugieren redundancia y posibles problemas en modelos).  

- üß≠ **¬øQu√© combinaciones de variables concentran la mayor varianza del dataset?**  
  (PCA identifica componentes principales y pesos de cada variable).  

En conjunto, estas herramientas permiten **descubrir dependencias, patrones ocultos y estructura global** del dataset, para guiar hip√≥tesis y preparar un modelado m√°s s√≥lido.
""")
    st.divider()
    st.subheader("Matriz de correlaci√≥n")
    c0, c1 = st.columns(2)
    c0.write("""Mide asociaci√≥n entre dos variables:
  - **Pearson (œÅ)**: relaci√≥n **lineal**; asume continuidad y es **sensible a outliers**.
  - **Spearman (œÅ‚Çõ)**: correlaci√≥n de **rangos**; captura relaciones **mon√≥tonas** (no requiere linealidad) y es m√°s **robusta a outliers**.
  - **Kendall (œÑ)**: basada en **pares concordantes/discordantes**; similar a Spearman, **m√°s conservadora** y estable en **muestras peque√±as**.
  - **Gu√≠a de magnitudes** (orientativa): |corr| < 0.3 d√©bil, 0.3‚Äì0.6 moderada, > 0.6 fuerte.
  - **Cuidado**: correlaci√≥n ‚â† causalidad; muchas pruebas simult√°neas ‚áí usa **Bonferroni/FDR**.
""")
    
    # ---------- Par√°metros globales ----------
    with c0.expander("Par√°metros", expanded=False):
        cA, cB = st.columns(2)
        corr_method = cA.selectbox("M√©todo correlaci√≥n", ["pearson","spearman","kendall"], index=0)
        cluster_heatmap = cB.checkbox("Clusterizar heatmap", True, help="Ordena variables por similitud |corr|.")
        sample_on = cA.checkbox("Muestrear para dispersi√≥n", True, help="Acelera gr√°ficos con muchos puntos.")
        max_points = cB.slider("Muestra m√°x.", 500, 50000, 5000, 500)

    df_plot = df.sample(n=min(max_points, len(df)), random_state=42) if sample_on else df

    @st.cache_data(show_spinner=False)
    def _corr_cached(df_num: pd.DataFrame, method: str) -> pd.DataFrame:
        return df_num.corr(method=method)

    def _cluster_corr(corr: pd.DataFrame) -> pd.DataFrame:
        try:
            from scipy.cluster.hierarchy import linkage, leaves_list
            dist = 1 - np.abs(corr.values)
            Z = linkage(dist, method="average")
            order = leaves_list(Z)
            cols = corr.columns[order]
            return corr.loc[cols, cols]
        except Exception:
            return corr

    # ---------- 1) Matriz de correlaci√≥n ----------

    if len(df.select_dtypes(np.number).columns) >= 2:
        num_cols = df.select_dtypes(np.number).columns.tolist()  # asegura consistencia si cambi√≥ antes
        corr = _corr_cached(df[num_cols], corr_method)
        if cluster_heatmap:
            corr = _cluster_corr(corr)

        fig_corr = px.imshow(
            corr, text_auto=True, aspect="auto",
            labels=dict(color="œÅ"),
            title=f"Correlaci√≥n num√©rica ({corr_method})"
        )
        c1.plotly_chart(fig_corr, use_container_width=True)
    else:
        c1.info("Necesitas al menos 2 variables num√©ricas.")

    c0.code(
            """corr = df[num_cols].corr(method=corr_method)
corr = _cluster_corr(corr) if cluster_heatmap else corr
fig = px.imshow(corr, text_auto=True, labels=dict(color="œÅ"))
st.plotly_chart(fig, use_container_width=True)""",
            language="python"
        )

    st.divider()

    # ---------- 2) Dispersi√≥n num√©rica vs num√©rica ----------
    c0, c1 = st.columns(2)
    c1.subheader("Dispersi√≥n num√©rica vs num√©rica")
    c1.write("""√ötil para ver **forma**, **no linealidad**, **heterocedasticidad**, **clusters** y **outliers**.  
    Si hay muchos puntos, usa **densidad (hex)** o **contornos** y colorea por **categor√≠a** (baja cardinalidad).""")
    if len(num_cols) >= 2:
        with c1:
            sc1, sc2, sc3 = st.columns(3)
            x_col = sc1.selectbox("X", num_cols, index=0, key="scat_x")
            y_opts = [c for c in num_cols if c != x_col] or num_cols
            y_col = sc2.selectbox("Y", y_opts, index=0, key="scat_y")

            # Color opcional por categ√≥rica de baja cardinalidad
            cat_cols = df.select_dtypes(include=["object","category","bool"]).columns.tolist()
            small_cats = [c for c in cat_cols if df[c].nunique(dropna=True) <= 25]
            color_opt = sc3.selectbox("Color por (opcional)", ["Ninguno"] + small_cats, index=0)
            color_arg = None if color_opt == "Ninguno" else color_opt

        kind = c1.radio("Tipo", ["Puntos","Densidad (hex)","Contornos"], horizontal=True)

        if kind == "Puntos":
            fig_s = px.scatter(df_plot, x=x_col, y=y_col, color=color_arg, opacity=0.7, hover_data=df_plot.columns)
        elif kind == "Densidad (hex)":
            fig_s = px.density_heatmap(df_plot, x=x_col, y=y_col, nbinsx=40, nbinsy=40,
                                       marginal_x="histogram", marginal_y="histogram")
        else:
            fig_s = px.density_contour(df_plot, x=x_col, y=y_col, color=color_arg,
                                       contours_coloring="fill", nbinsx=40, nbinsy=40)

        c0.plotly_chart(fig_s, use_container_width=True)
    else:
        c0.info("Selecciona al menos dos columnas num√©ricas.")

    c1.code(
            """fig = px.scatter(df_plot, x=x_col, y=y_col, color=color_arg, opacity=0.7)
st.plotly_chart(fig, use_container_width=True)""",
            language="python"
        )

    st.divider()

    # ---------- 3) Num√©rica vs categ√≥rica (violin + box + Top-N) ----------
    c0, c1 = st.columns(2)
    c0.subheader("Distribuci√≥n num√©rica por categor√≠a")
    c0.write("**Violin/box** para distribuci√≥n y outliers por grupo; limita a **Top-N** categor√≠as y a√±ade **resumen** (count/mean/median/std).")

    if cat_cols and num_cols:
        with c0:
            sc1, sc2 = st.columns(2)
            cat_sel = sc1.selectbox("Categor√≠a", cat_cols, key="cat_box")
            num_sel = sc2.selectbox("Num√©rica", num_cols, key="num_box")
        top_n = c0.slider("Top-N categor√≠as por frecuencia", 3, 30, 12)

        top_cats = df_plot[cat_sel].value_counts(dropna=False).head(top_n).index
        data_cat = df_plot[df_plot[cat_sel].isin(top_cats)].copy()

        fig_violin = px.violin(data_cat, x=cat_sel, y=num_sel, box=True, points="outliers",
                               title=f"{num_sel} por {cat_sel} (Top-{top_n})")
        c1.plotly_chart(fig_violin, use_container_width=True)

        summary = (
            data_cat.groupby(cat_sel, dropna=False)[num_sel]
            .agg(count="count", mean="mean", median="median", std="std")
            .sort_values("mean", ascending=False)
        )
        c1.dataframe(summary, use_container_width=True)
    else:
        c1.info("Necesitas ‚â•1 columna categ√≥rica y ‚â•1 num√©rica.")

    c0.code(
            """top_cats = df_plot[cat_sel].value_counts().head(top_n).index
data_cat = df_plot[df_plot[cat_sel].isin(top_cats)]
fig = px.violin(data_cat, x=cat_sel, y=num_sel, box=True, points="outliers")
st.plotly_chart(fig, use_container_width=True)""",
            language="python"
        )

    st.divider()

    # ---------- 4) PCA ----------
    c0, c1 = st.columns(2)
    c1.subheader("PCA (2 componentes)")
    c1.markdown("""
**¬øQu√© responde PCA?**  
- ¬øCu√°ntas **dimensiones latentes** explican la mayor parte de la variaci√≥n?
- ¬øQu√© **variables** impulsan cada componente (loadings)?
- ¬øExisten **cl√∫steres** o **separaci√≥n** entre grupos en 2D?

**C√≥mo leer los resultados**  
- **Varianza explicada**: PC1 explica x%, PC2 y% (suma ‚âà informaci√≥n retenida en 2D). 
- **Loadings**: valores altos (¬±) ‚áí variables **m√°s influyentes**. Un mismo signo en varias variables sugiere un **eje com√∫n** (p. ej., ‚Äútama√±o/ingreso‚Äù).  
- **Gr√°fico PC1 vs PC2**: busca **grupos**, **gradientes** (color por categor√≠a), **outliers**.

**Buenas pr√°cticas**  
- Solo **num√©ricas** y **sin NaNs**; aplicar **StandardScaler** (z-score).  
- Revisa **outliers** antes: pueden dominar componentes.  
- PC1/PC2 con muy baja varianza ‚áí quiz√° requieras **m√°s PCs** o t√©cnicas no lineales (t-SNE/UMAP).

**Limitaciones**  
- Es **lineal** y **no supervisado**: no maximiza separaci√≥n por target.  
- Componentes son combinaciones; la **interpretaci√≥n** depende de los **loadings**.
""")
    c0.code(
                """X = df[num_cols].dropna()
Xs = StandardScaler().fit_transform(X)
pca = PCA(n_components=2).fit(Xs)
scores = pca.transform(Xs)
explained = pca.explained_variance_ratio_
pca_df = pd.DataFrame(scores, columns=["PC1","PC2"], index=X.index)
fig = px.scatter(pca_df, x="PC1", y="PC2", title=f"Var explicada: {explained[:2].sum():.1%}")
st.plotly_chart(fig, use_container_width=True)""",
                language="python"
            )
    if len(num_cols) >= 2:
        X = df[num_cols].dropna(axis=0)
        idx = X.index

        # Color opcional por categ√≥rica de baja cardinalidad (alineada al √≠ndice)
        small_cats_full = [c for c in cat_cols if df[c].nunique(dropna=True) <= 25] if cat_cols else []
        color_pca_opt = c0.selectbox("Color por (opcional)", ["Ninguno"] + small_cats_full, index=0, key="pca_color")
        color_series = None if color_pca_opt == "Ninguno" else df.loc[idx, color_pca_opt]

        Xs = StandardScaler().fit_transform(X.values)
        pca = PCA(n_components=2).fit(Xs)
        scores = pca.transform(Xs)
        exp = pca.explained_variance_ratio_

        pca_df = pd.DataFrame(scores, columns=["PC1","PC2"], index=idx)
        if color_series is not None:
            pca_df[color_pca_opt] = color_series.values

        fig_pca = px.scatter(
            pca_df, x="PC1", y="PC2",
            color=color_pca_opt if color_series is not None else None,
            title=f"PCA: PC1 {exp[0]:.1%} ¬∑ PC2 {exp[1]:.1%} ¬∑ Total {(exp[:2].sum()):.1%}"
        )
        c0.plotly_chart(fig_pca, use_container_width=True)
        
        loadings = pd.DataFrame(pca.components_.T, index=num_cols, columns=["PC1","PC2"])
        cL, cR = st.columns(2)
        cL.plotly_chart(px.bar(loadings["PC1"].abs().sort_values(ascending=False).head(12),
                               title="Top |loading| PC1", orientation="h"), use_container_width=True)
        cR.plotly_chart(px.bar(loadings["PC2"].abs().sort_values(ascending=False).head(12),
                               title="Top |loading| PC2", orientation="h"), use_container_width=True)

    else:
        c0.info("PCA requiere ‚â•2 columnas num√©ricas.")


# =========================
# üö® Outliers & Anomal√≠as
# =========================
with t4:
    st.subheader("üö® Outliers & Anomal√≠as")

    st.markdown("""
    **¬øQu√© responde este paso?**
    - ¬øCu√°ntas y cu√°les observaciones caen fuera de lo esperado por **dispersi√≥n**?
    - ¬øLos extremos son puntuales o sistem√°ticos por subgrupo?
    - ¬øExisten **anomal√≠as multivariantes** que no se ven univariadamente?

    **Teor√≠a r√°pida**
    - **IQR (Tukey)**: marca outliers fuera de \\[Q1 ‚àí k¬∑IQR, Q3 + k¬∑IQR\\]. Sencillo y robusto a no-normalidad.
    - **z-score robusto (MAD)**: usa mediana y **MAD** ‚Üí menos sensible a outliers que el z-score cl√°sico.
    - **Percentiles**: define umbrales emp√≠ricos (p.ej., 1% y 99%) cuando la forma es muy rara o multimodal.
    - **Multivariante** (IsolationForest): detecta puntos raros considerando **todas las num√©ricas a la vez**.
    """)

    num_cols = df.select_dtypes(np.number).columns.tolist()
    if not num_cols:
        st.info("No hay columnas num√©ricas disponibles.")
    else:
        # ---------- Par√°metros ----------
        c0, c1, c2 = st.columns(3)
        col_out = c0.selectbox("Variable num√©rica", num_cols, key="outlier_var")
        method = c1.radio("M√©todo", ["IQR", "Z robusto (MAD)", "Percentiles"], horizontal=True, key="out_method")

        # Controles dependientes del m√©todo
        k = None; z_thr = None; q_lo = None; q_hi = None
        if method == "IQR":
            k = c2.slider("k (IQR)", 1.0, 5.0, 1.5, 0.1)
        elif method == "Z robusto (MAD)":
            z_thr = c2.slider("|z| robusto", 2.0, 6.0, 3.5, 0.1)
        else:
            c2.empty()
            c3, c4 = st.columns(2)
            q_lo = c3.slider("Percentil inferior", 0.0, 10.0, 1.0, 0.1)
            q_hi = c4.slider("Percentil superior", 90.0, 100.0, 99.0, 0.1)

        serie = df[col_out].astype(float)
        serie_clean = serie.dropna()
        n_valid = int(serie_clean.shape[0])

        # ---------- C√°lculo de outliers ----------
        mask = pd.Series(False, index=df.index)
        lo = hi = None
        score = pd.Series(np.nan, index=df.index)  # "fuerza" del outlier para ordenar

        if n_valid == 0:
            st.warning("La variable seleccionada solo contiene nulos.")
        else:
            if method == "IQR":
                Q1, Q3 = serie_clean.quantile([0.25, 0.75])
                IQR = Q3 - Q1
                lo, hi = Q1 - k * IQR, Q3 + k * IQR
                mask = (serie < lo) | (serie > hi)
                # distancia relativa al rango IQR para ordenar
                med = float(serie_clean.median())
                score = (serie - med).abs() / (IQR if IQR != 0 else 1.0)

            elif method == "Z robusto (MAD)":
                med = float(serie_clean.median())
                mad = float((serie_clean - med).abs().median())
                if mad == 0:
                    st.info("MAD = 0; no se puede calcular z robusto. Prueba IQR o percentiles.")
                    mask = pd.Series(False, index=df.index)
                else:
                    robust_z = (serie - med) / (1.4826 * mad)
                    score = robust_z.abs()
                    mask = score > z_thr

            else:  # Percentiles
                lo = float(np.percentile(serie_clean, q_lo))
                hi = float(np.percentile(serie_clean, q_hi))
                mask = (serie < lo) | (serie > hi)
                med = float(serie_clean.median())
                score = (serie - med).abs()  # distancia absoluta a la mediana

            n_out = int(mask.fillna(False).sum())
            pct_out = (n_out / n_valid * 100) if n_valid > 0 else 0.0

            # ---------- Visualizaciones ----------
            st.subheader("Boxplot (vista r√°pida)")
            st.plotly_chart(px.box(df, y=col_out, points="outliers",
                                   title=f"Boxplot: {col_out}"),
                            use_container_width=True)

            st.subheader("Distribuci√≥n y umbrales")
            fig_hist = px.histogram(serie_clean, nbins=50, opacity=0.85,
                                    title=f"Histograma: {col_out}")
            # L√≠neas de umbral
            try:
                if method in ["IQR", "Percentiles"]:
                    fig_hist.add_vline(x=lo, line_dash="dash", annotation_text=f"lo={lo:,.3g}")
                    fig_hist.add_vline(x=hi, line_dash="dash", annotation_text=f"hi={hi:,.3g}")
                elif method == "Z robusto (MAD)":
                    fig_hist.add_vline(x=med, line_dash="dot", annotation_text=f"mediana={med:,.3g}")
            except Exception:
                pass
            st.plotly_chart(fig_hist, use_container_width=True)

            # ---------- M√©tricas + tabla ----------
            cA, cB = st.columns(2)
            cA.metric("Outliers detectados", f"{n_out:,}")
            cB.metric("% sobre v√°lidos", f"{pct_out:.2f}%")

            st.subheader("Observaciones at√≠picas (Top-N por 'fuerza')")
            top_n = st.slider("Top-N a mostrar", 10, 200, 50, 10)
            out_df = (
                df.loc[mask].assign(_out_score=score[mask])
                .sort_values("_out_score", ascending=False)
                .head(top_n)
            )
            st.dataframe(out_df, use_container_width=True)

            # Descargar CSV de outliers
            if n_out > 0:
                csv_buf = io.StringIO()
                df.loc[mask].assign(_out_score=score[mask]).to_csv(csv_buf, index=False)
                st.download_button(
                    "‚¨áÔ∏è Descargar outliers (CSV)",
                    data=csv_buf.getvalue(),
                    file_name=f"outliers_{col_out}_{method.replace(' ','_')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )

            # ---------- C√≥digo (expander) ----------
            with st.expander("Ver c√≥digo (detecci√≥n univariante)"):
                st.code(
                    """# IQR
Q1, Q3 = x.quantile([0.25, 0.75]); IQR = Q3 - Q1
lo, hi = Q1 - 1.5*IQR, Q3 + 1.5*IQR
mask_iqr = (x < lo) | (x > hi)

# z robusto (MAD)
med = x.median(); mad = (x - med).abs().median()
robust_z = (x - med) / (1.4826 * mad)
mask_mad = robust_z.abs() > 3.5

# percentiles
lo = np.percentile(x.dropna(), 1); hi = np.percentile(x.dropna(), 99)
mask_pct = (x < lo) | (x > hi)""",
                    language="python"
                )

        # ---------- Multivariante opcional ----------
        st.divider()
        st.subheader("üîÄ Detecci√≥n multivariante (IsolationForest) ‚Äî opcional")
        st.caption("√ötil cuando un punto no es extremo en 1D, pero s√≠ al combinar varias num√©ricas.")

        enable_if = st.checkbox("Activar IsolationForest", value=False)
        if enable_if:
            try:
                from sklearn.ensemble import IsolationForest
                from sklearn.preprocessing import StandardScaler
                from sklearn.decomposition import PCA

                feats = st.multiselect("Variables num√©ricas a considerar", num_cols,
                                       default=num_cols[:min(5, len(num_cols))])
                if len(feats) == 0:
                    st.info("Selecciona al menos 1 variable num√©rica.")
                else:
                    c1, c2 = st.columns(2)
                    contamination = c1.slider("Proporci√≥n esperada de anomal√≠as", 0.01, 0.20, 0.05, 0.01)
                    n_estimators = c2.slider("√Årboles (n_estimators)", 50, 400, 200, 50)

                    X = df[feats].dropna()
                    if X.empty:
                        st.info("No hay filas completas para las variables seleccionadas.")
                    else:
                        iso = IsolationForest(
                            contamination=contamination,
                            n_estimators=n_estimators,
                            random_state=42
                        ).fit(X)

                        pred = iso.predict(X)  # -1 = outlier
                        score_if = iso.decision_function(X)  # menor = m√°s raro
                        res = df.loc[X.index].copy()
                        res["_anomaly_if"] = (pred == -1)
                        res["_if_score"] = score_if

                        n_anom = int(res["_anomaly_if"].sum())
                        pct_anom = n_anom / res.shape[0] * 100
                        cA, cB = st.columns(2)
                        cA.metric("Anomal√≠as (IForest)", f"{n_anom:,}")
                        cB.metric("% sobre v√°lidos", f"{pct_anom:.2f}%")

                        # Vista en PCA 2D para interpretar
                        Xs = StandardScaler().fit_transform(X)
                        p2 = PCA(n_components=2).fit_transform(Xs)
                        plot_df = pd.DataFrame({
                            "PC1": p2[:, 0],
                            "PC2": p2[:, 1],
                            "Anomal√≠a": np.where(res["_anomaly_if"], "S√≠", "No")
                        }, index=X.index)
                        fig_if = px.scatter(
                            plot_df, x="PC1", y="PC2", color="Anomal√≠a",
                            title="IsolationForest en espacio PCA (2D)",
                            opacity=0.85
                        )
                        st.plotly_chart(fig_if, use_container_width=True)

                        st.subheader("Muestras m√°s an√≥malas (seg√∫n score)")
                        show_n = st.slider("Mostrar Top-N", 10, 200, 50, 10, key="if_topn")
                        st.dataframe(
                            res.sort_values("_if_score").head(show_n),
                            use_container_width=True
                        )

                        with st.expander("Ver c√≥digo (IsolationForest)"):
                            st.code(
                                """from sklearn.ensemble import IsolationForest
iso = IsolationForest(contamination=0.05, n_estimators=200, random_state=42).fit(X)
pred = iso.predict(X)          # -1 = outlier
score = iso.decision_function(X)  # menor = m√°s raro
mask_if = pred == -1""",
                                language="python"
                            )
            except Exception as e:
                st.info(f"No se pudo activar IsolationForest: {e}")

    # ---------- Sugerencias de tratamiento ----------
    st.divider()
    with st.expander("üíä Tratamiento: opciones habituales"):
        st.markdown("""
        - **Inspecci√≥n manual** y correcci√≥n si es error de captura.
        - **Winsorizaci√≥n (clip)** en percentiles (p.ej., 1%‚Äì99%).
        - **Transformaciones** (log/sqrt/Box-Cox) si hay sesgo fuerte.
        - **Modelado robusto** (√°rboles/boosting, regularizaci√≥n) si no quieres remover.
        - **Eliminar** casos extremos solo si justificas su falta de representatividad.

        **Snippet winsorizaci√≥n:**
        ```python
        lo, hi = x.quantile([0.01, 0.99])
        x_wins = x.clip(lo, hi)
        ```
        """)
# =========================
# üß© Categ√≥ricas & Balance de clases
# =========================
with t5:
    st.subheader("üß© Categ√≥ricas & Balance de clases")

    # Asegura listas de columnas (recalcula por si cambi√≥ df)
    cat_cols = df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()
    num_cols = df.select_dtypes(include=np.number).columns.tolist()

    st.markdown("""
    **¬øQu√© responde este paso?**  
    - ¬øCu√°les son las categor√≠as m√°s **frecuentes** (Top-N) y cu√°nto representan (%)?  
    - ¬øHay **desbalance** fuerte entre clases que pueda afectar el modelado?  
    - ¬øC√≥mo contribuye cada categor√≠a a un **valor num√©rico total** (treemap)?  
    - ¬øC√≥mo se comporta un **objetivo num√©rico** dentro de cada categor√≠a (medias/medianas)?  
    """)

    st.divider()

    # =========================
    # 1) Treemap (Categor√≠a -> Valor num√©rico)
    # =========================
    if cat_cols and num_cols:
        c0, c1 = st.columns(2)

        c0.subheader("üå≥ Treemap (Categor√≠a ‚Üí Valor)")
        c0.write(
                "El **treemap** muestra la contribuci√≥n de cada categor√≠a a un **total num√©rico** "
                "(√°rea proporcional al valor). √ötil para detectar categor√≠as dominantes."
            )
        c0.code(
                    """cat_t = st.selectbox("Categor√≠a", cat_cols, key="t5_treemap_cat")
val_t = st.selectbox("Valor num√©rico", num_cols, key="t5_treemap_val")
fig_tm = px.treemap(df, path=[cat_t], values=val_t, title=f"Treemap: {cat_t} ‚Üí {val_t}")
st.plotly_chart(fig_tm, use_container_width=True)""",
                    language="python"
                )

        with c0:
            sc0, sc1 = st.columns(2)
            cat_t = sc0.selectbox("Categor√≠a", cat_cols, key="t5_treemap_cat")
            val_t = sc1.selectbox("Valor num√©rico", num_cols, key="t5_treemap_val")
            fig_tm = px.treemap(df, path=[cat_t], values=val_t, title=f"Treemap: {cat_t} ‚Üí {val_t}")
        
        c1.plotly_chart(fig_tm, use_container_width=True)

    else:
        st.info("Para el Treemap se requiere al menos una columna categ√≥rica y una num√©rica.")

    st.divider()

    # =========================
    # 2) Distribuci√≥n de categor√≠as (Top-N + 'Otros') + % robusto
    # =========================
    if cat_cols:
        c0, c1 = st.columns(2)

        c1.subheader("üìä Distribuci√≥n de categor√≠as (Top-N + 'Otros')")
        c1.write(
                "Mostramos **Top-N** categor√≠as por frecuencia y agrupamos el resto en **'Otros'** "
                "para mantener legibilidad. Adem√°s, calculamos el **%** de cada categor√≠a."
            )

        c1.code(
                    """col_cat = st.selectbox("Variable categ√≥rica", cat_cols, key="t5_cat_var")
top_n = st.slider("Top-N categor√≠as", 3, 30, 10, key="t5_topn")

# Conteos robustos
vc = df[col_cat].value_counts(dropna=False)          # Serie de conteos
top = vc.head(top_n)
others = vc.iloc[top_n:].sum()

plot_s = top.copy()
if others > 0:
    plot_s.loc["Otros"] = int(others)

# DataFrame final sin duplicados y con num√©ricos
counts_df = (
    plot_s.rename("count")
         .reset_index()
         .rename(columns={"index": col_cat})
)
counts_df["count"] = pd.to_numeric(counts_df["count"], errors="coerce").fillna(0)

total = counts_df["count"].sum(min_count=1)
counts_df["pct"] = (counts_df["count"] / (total if total else 1)) * 100

st.dataframe(counts_df, use_container_width=True)

fig_bar = px.bar(
    counts_df.sort_values("count", ascending=False),
    x=col_cat, y="count",
    text=counts_df["pct"].map(lambda x: f"{x:.1f}%"),
    title=f"Distribuci√≥n: {col_cat}",
    labels={"count": "Frecuencia"}
)
fig_bar.update_traces(textposition="outside", cliponaxis=False)
st.plotly_chart(fig_bar, use_container_width=True)""",
                    language="python"
                )

        with c0:
            # --- UI y c√°lculo robusto ---
            col_cat = st.selectbox("Variable categ√≥rica", cat_cols, key="t5_cat_var")
            top_n = st.slider("Top-N categor√≠as", 3, 30, 10, key="t5_topn")

            vc = df[col_cat].value_counts(dropna=False)   # Serie
            top = vc.head(top_n)
            others = vc.iloc[top_n:].sum()

            plot_s = top.copy()
            if others > 0:
                plot_s.loc["Otros"] = int(others)

            counts_df = (
                plot_s.rename("count")
                     .reset_index()
                     .rename(columns={"index": col_cat})
            )
            counts_df["count"] = pd.to_numeric(counts_df["count"], errors="coerce").fillna(0)

            total = counts_df["count"].sum(min_count=1)
            counts_df["pct"] = (counts_df["count"] / (total if total else 1)) * 100

            st.dataframe(counts_df, use_container_width=True)

            fig_bar = px.bar(
                counts_df.sort_values("count", ascending=False),
                x=col_cat, y="count",
                text=counts_df["pct"].map(lambda x: f"{x:.1f}%"),
                title=f"Distribuci√≥n: {col_cat}",
                labels={"count": "Frecuencia"}
            )
            fig_bar.update_traces(textposition="outside", cliponaxis=False)
            st.plotly_chart(fig_bar, use_container_width=True)

        # ===== Balance r√°pido de clases =====
        # Basado en los conteos "puros" (sin 'Otros') para evaluar desbalance
        major_cat = vc.index[0] if len(vc) else None
        if major_cat is not None:
            major_count = int(vc.iloc[0])
            total_all = int(vc.sum())
            major_pct = (major_count / total_all * 100) if total_all else 0.0

            with c0:
                cA, cB, cC = st.columns(3)
                cA.metric("Categor√≠a mayoritaria", str(major_cat))
                cB.metric("Frecuencia mayoritaria", f"{major_count:,}")
                cC.metric("% mayoritario", f"{major_pct:.1f}%")

            if major_pct >= 80:
                st.warning("Desbalance fuerte (‚â•80/20). Considera t√©cnicas: reponderaci√≥n, undersampling/oversampling, m√©tricas por clase.")

    else:
        st.info("No hay columnas categ√≥ricas disponibles.")

    st.divider()

    # =========================
    # 3) Relaci√≥n con objetivo num√©rico (media/mediana por categor√≠a)
    # =========================
    if cat_cols and num_cols:
        c0, c1 = st.columns(2)

        with c0:
            st.subheader("üéØ Objetivo num√©rico por categor√≠a")
            st.write(
                "Compara un objetivo num√©rico entre categor√≠as usando **media** o **mediana** "
                "(robusta a outliers). Para legibilidad, se limita a las **Top-N** categor√≠as."
            )
            st.code(
                    """col_cat2 = st.selectbox("Categor√≠a", cat_cols, key="t5_target_cat")
target = st.selectbox("Objetivo num√©rico", num_cols, key="t5_target_val")
agg_fn = st.selectbox("Agregaci√≥n", ["mean", "median", "sum"], index=0, key="t5_target_agg")
top_n_target = st.slider("Top-N (por frecuencia)", 3, 30, 10, key="t5_target_topn")

vc2 = df[col_cat2].value_counts(dropna=False)
keep = vc2.head(top_n_target).index
data_f = df[df[col_cat2].isin(keep)]

agg = getattr(data_f.groupby(col_cat2, dropna=False)[target], agg_fn)().reset_index()
agg = agg.sort_values(by=agg.columns[1], ascending=False)

fig_t = px.bar(agg, x=col_cat2, y=agg.columns[1],
               title=f"{target} ({agg_fn}) por {col_cat2}",
               labels={agg.columns[1]: f"{target} ({agg_fn})"})
st.plotly_chart(fig_t, use_container_width=True)""",
                    language="python"
                )

        with c1:
            sc0, sc1, sc2 = st.columns(3)
            col_cat2 = sc0.selectbox("Categor√≠a", cat_cols, key="t5_target_cat")
            target = sc1.selectbox("Objetivo num√©rico", num_cols, key="t5_target_val")
            agg_fn = sc2.selectbox("Agregaci√≥n", ["mean", "median", "sum"], index=0, key="t5_target_agg")
            top_n_target = st.slider("Top-N (por frecuencia)", 3, 30, 10, key="t5_target_topn")

            vc2 = df[col_cat2].value_counts(dropna=False)
            keep = vc2.head(top_n_target).index
            data_f = df[df[col_cat2].isin(keep)]

            agg_series = getattr(data_f.groupby(col_cat2, dropna=False)[target], agg_fn)()
            agg = agg_series.reset_index().sort_values(by=target, ascending=False)

            fig_t = px.bar(
                agg, x=col_cat2, y=target,
                title=f"{target} ({agg_fn}) por {col_cat2}",
                labels={target: f"{target} ({agg_fn})"}
            )
            st.plotly_chart(fig_t, use_container_width=True)
    else:
        st.info("Para esta secci√≥n se requiere al menos una categ√≥rica y una num√©rica.")

# =========================
# ‚è≥ Series de tiempo (t6 completo y robusto)
# =========================
with t6:
    st.subheader("‚è≥ Series de tiempo")

    # ---------- Helpers ----------
    def _is_parseable_datetime(s: pd.Series, min_ok: float = 0.5) -> bool:
        """True si >= min_ok se puede parsear a datetime."""
        if np.issubdtype(s.dtype, np.datetime64):
            return True
        try:
            parsed = pd.to_datetime(s, errors="coerce", infer_datetime_format=True)
            return parsed.notna().mean() >= min_ok
        except Exception:
            return False

    def _ensure_datetime_col(df_: pd.DataFrame, col: str) -> pd.Series:
        """Convierte a datetime si no lo es."""
        if np.issubdtype(df_[col].dtype, np.datetime64):
            return df_[col]
        return pd.to_datetime(df_[col], errors="coerce", infer_datetime_format=True)

    def _looks_like_id(name: str) -> bool:
        """Heur√≠stica simple para excluir identificadores num√©ricos."""
        n = str(name).lower()
        return (
            n == "id" or n == "index" or
            n.endswith("_id") or n.startswith("id_") or
            n.endswith(" id") or n.startswith("id ")
        )

    # ---------- Detecci√≥n de columnas de fecha ----------
    date_candidates = [c for c in df.columns if _is_parseable_datetime(df[c])]

    if not date_candidates:
        st.info("No se detectaron columnas de fecha.")
    else:
        # ---------- Layout 2 columnas ----------
        c0, c1 = st.columns(2)

        # ===== Lado izquierdo: teor√≠a + par√°metros =====
        with c0:
            st.subheader("Qu√© responde")
            st.write(
                "- ¬øC√≥mo evoluciona la m√©trica en el tiempo (tendencia/estacionalidad)?\n"
                "- ¬øC√≥mo afectan la **frecuencia** y la **agregaci√≥n** (sum/mean/‚Ä¶)?\n"
                "- ¬øCu√°l es el **cambio %** per√≠odo a per√≠odo o **interanual (YoY)**?\n"
                "- ¬øHay diferencias por **categor√≠a** (l√≠neas m√∫ltiples)?"
            )

            st.subheader("Par√°metros")
            date_col = st.selectbox("Columna de fecha", date_candidates, key="ts_date_col")

        # Construir df_ts con fecha v√°lida ANTES de elegir la m√©trica
        df_ts = df.copy()
        df_ts[date_col] = _ensure_datetime_col(df_ts, date_col)
        df_ts = df_ts.dropna(subset=[date_col]).sort_values(by=date_col)

        # Num√©ricas v√°lidas (excluye identificadores)
        num_cols_ts = [
            c for c in df_ts.select_dtypes(np.number).columns
            if c != date_col and not _looks_like_id(c)
        ]

        # Categ√≥ricas de baja cardinalidad
        cat_cols_all = df_ts.select_dtypes(include=["object", "category", "bool"]).columns.tolist()
        small_cats = [c for c in cat_cols_all if df_ts[c].nunique(dropna=True) <= 12]

        with c0:
            if not num_cols_ts:
                st.warning("No hay columnas num√©ricas v√°lidas para series de tiempo (excluyendo identificadores como 'id').")
            else:
                val_col = st.selectbox("Variable num√©rica", num_cols_ts, key="ts_val_col")
                split_col = st.selectbox("Dividir por categor√≠a (opcional)", ["Ninguno"] + small_cats, key="ts_split")

                cA, cB, cC = st.columns(3)
                freq_lbl = cA.selectbox("Frecuencia", ["D (d√≠a)", "W (semana)", "M (mes)", "Q (trimestre)", "Y (a√±o)"], index=2, key="ts_freq")
                agg_fn_name = cB.selectbox("Agregaci√≥n", ["sum", "mean", "median", "max", "min"], index=0, key="ts_agg")
                fill_mode = cC.selectbox("Relleno faltantes", ["Ninguno", "Forward-fill", "Cero"], index=0, key="ts_fill")

                cD, cE = st.columns(2)
                pct_kind = cD.selectbox("% cambio", ["Ninguno", "Per√≠odo a per√≠odo", "Interanual (YoY)"], index=0, key="ts_pct")
                show_ma = cE.checkbox("Media m√≥vil", value=True, key="ts_ma")
                ma_window = st.slider("Ventana MA (periodos)", 2, 60, 12, key="ts_ma_win") if show_ma else None

                st.code(
                        """# Resample b√°sico (una serie)
ts = (df.set_index(date_col).sort_index()[val_col]
        .resample('M').sum()
        .reset_index())""",
                        language="python"
                    )

        # ===== Lado derecho: resultados =====
        with c1:
            if not num_cols_ts:
                st.stop()

            freq_map = {"D (d√≠a)": "D", "W (semana)": "W", "M (mes)": "M", "Q (trimestre)": "Q", "Y (a√±o)": "Y"}
            f = freq_map[freq_lbl]

            if split_col == "Ninguno":
                # --- Serie √∫nica ---
                s = (
                    df_ts.set_index(date_col)
                         .loc[:, val_col]
                         .resample(f)
                         .agg(agg_fn_name)
                )
                if fill_mode == "Forward-fill":
                    s = s.ffill()
                elif fill_mode == "Cero":
                    s = s.fillna(0)

                ts = s.reset_index().rename(columns={val_col: val_col})

                # Media m√≥vil
                ys = [val_col]
                if show_ma:
                    ts["MA"] = ts[val_col].rolling(int(ma_window)).mean()
                    ys.append("MA")

                fig_ts = px.line(
                    ts, x=date_col, y=ys,
                    title=f"{val_col} ‚Äî {agg_fn_name} por {f}",
                    labels={date_col: "Fecha"}
                )
                st.plotly_chart(fig_ts, use_container_width=True)

                # % cambio
                if pct_kind != "Ninguno":
                    if pct_kind == "Per√≠odo a per√≠odo":
                        periods = 1
                    else:
                        shift_map = {"M": 12, "Q": 4, "W": 52, "D": 365, "Y": 1}
                        periods = shift_map.get(f, 1)
                    ts["pct_change"] = ts[val_col].pct_change(periods=periods) * 100
                    fig_pct = px.line(
                        ts, x=date_col, y="pct_change",
                        title=("Cambio % per√≠odo a per√≠odo" if periods == 1 else "Cambio % interanual (YoY)"),
                        labels={"pct_change": "%"}
                    )
                    st.plotly_chart(fig_pct, use_container_width=True)

                st.dataframe(ts.tail(12), use_container_width=True)

                st.code(
                        """# % cambio
periods = 12  # ejemplo mensual YoY
ts["pct_change"] = ts[val_col].pct_change(periods=periods) * 100""",
                        language="python"
                    )

            else:
                # --- M√∫ltiples series por categor√≠a ---
                grp = (
                    df_ts.groupby([pd.Grouper(key=date_col, freq=f), split_col])[val_col]
                         .agg(agg_fn_name)
                         .reset_index()
                )

                # Relleno por grupo (no crea nuevas fechas, solo rellena NaN existentes)
                if fill_mode != "Ninguno":
                    def _fill(g):
                        if fill_mode == "Forward-fill":
                            g[val_col] = g[val_col].ffill()
                        elif fill_mode == "Cero":
                            g[val_col] = g[val_col].fillna(0)
                        return g
                    grp = grp.groupby(split_col, group_keys=False).apply(_fill)

                # Media m√≥vil por grupo
                if show_ma:
                    grp["MA"] = grp.groupby(split_col)[val_col].transform(lambda s: s.rolling(int(ma_window)).mean())

                fig_multi = px.line(
                    grp, x=date_col, y=val_col, color=split_col,
                    title=f"{val_col} ‚Äî {agg_fn_name} por {f} (por {split_col})"
                )
                st.plotly_chart(fig_multi, use_container_width=True)

                if show_ma:
                    fig_ma = px.line(
                        grp, x=date_col, y="MA", color=split_col,
                        title=f"Media m√≥vil ({ma_window}) por {split_col}"
                    )
                    st.plotly_chart(fig_ma, use_container_width=True)

                # % cambio por grupo
                if pct_kind != "Ninguno":
                    if pct_kind == "Per√≠odo a per√≠odo":
                        periods = 1
                    else:
                        shift_map = {"M": 12, "Q": 4, "W": 52, "D": 365, "Y": 1}
                        periods = shift_map.get(f, 1)
                    grp["pct_change"] = grp.groupby(split_col)[val_col].pct_change(periods=periods) * 100
                    fig_pctm = px.line(
                        grp, x=date_col, y="pct_change", color=split_col,
                        title=("Cambio % per√≠odo a per√≠odo" if periods == 1 else "Cambio % interanual (YoY)"),
                        labels={"pct_change": "%"}
                    )
                    st.plotly_chart(fig_pctm, use_container_width=True)

                st.dataframe(
                    grp.sort_values(date_col).groupby(split_col).tail(6),
                    use_container_width=True
                )

                st.code(
                        """grp = (df
    .groupby([pd.Grouper(key=date_col, freq='M'), split_col])[val_col]
    .sum()
    .reset_index())
# Rolling por grupo
grp["MA"] = grp.groupby(split_col)[val_col].transform(lambda s: s.rolling(12).mean())""",
                        language="python"
                    )

        c0.write("üí° Tips & buenas pr√°cticas")
        c0.markdown(
                "- Si la fecha no es `datetime`, se **parsea autom√°ticamente**.\n"
                "- **Frecuencia** cambia unidad de an√°lisis (D/W/M/Q/Y) y **agregaci√≥n** define el resumen.\n"
                "- **Forward-fill** es √∫til para tasas; **Cero** para conteos cuando faltan periodos.\n"
                "- **YoY** requiere al menos un a√±o de historial a la misma frecuencia.\n"
                "- Para descomposici√≥n estacional: `statsmodels.tsa.seasonal_decompose` (opcional)."
            )


